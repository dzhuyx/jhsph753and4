<!DOCTYPE html>
<html>
<head>
  <title>Prediction III</title>
  <meta charset="utf-8">
  <meta name="description" content="Prediction III">
  <meta name="author" content="Jeffrey Leek">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="../../librariesNew/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  
  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../librariesNew/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="../../librariesNew/frameworks/io2012/js/slides" 
    src="../../librariesNew/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="../../assets/img/bloomberg_shield.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Prediction III</h1>
    <h2></h2>
    <p>Jeffrey Leek<br/>Johns Hopkins Bloomberg School of Public Health</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Pro tip</h2>
  </hgroup>
  <article data-timings="">
    <p>Academia is a weird place. The main criteria you are evaluated on is research. Everything we learn about in this class is focused on that. But in reality you spend a huge amount of time doing the following:</p>

<ul>
<li>Meetings</li>
<li>Giving talks</li>
<li>Advising students</li>
<li>Teaching</li>
<li>Reviewing (grants, papers, etc.)</li>
<li>Interacting with collaobrators </li>
</ul>

<p>These skills are often secondary in graduate programs but become hugely important very quickly when you become a faculty member. Learning how to do them well early is a fast way to make the transition to faculty dramatically easier. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Papers of the day</h2>
  </hgroup>
  <article data-timings="">
    <p><a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">Bagging predictors</a></p>

<p><a href="http://cseweb.ucsd.edu/%7Eyfreund/papers/IntroToBoosting.pdf">A short introduction to boosting</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>New pro tip</h2>
  </hgroup>
  <article data-timings="">
    <p>The absolute best way to have maximum impact:</p>

<ol>
<li>Find a scientific problem that hasn&#39;t been solved with data (by far hardest part)</li>
<li>Collect data/partner up with someone with data for that problem. </li>
<li>Create a good solution to the problem </li>
<li>Only invent new methods if you must</li>
<li>Write software and document the hell out of it</li>
<li>Respond to users and update as needed</li>
<li>Don&#39;t get (meanly) competitive</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>New paper of the day</h2>
  </hgroup>
  <article data-timings="">
    <p><a href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/">Does researching casual marijuana use cause brain abnormalities?</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>An example of least parametric to most</h2>
  </hgroup>
  <article data-timings="">
    <p><center> KNN -&gt; Logistic Regression -&gt; LDA -&gt; Fully Bayesian model </center></p>

<ul>
<li>K nearest neighbors</li>
<li>Logistic regression</li>
<li>Linear discriminant analysis</li>
<li>Fully Bayesian model</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>KNN neighbors</h2>
  </hgroup>
  <article data-timings="">
    <p>Basic idea</p>

<p>\[\hat{Y}(x) = \frac{1}{k}\sum_{x_i \in N_k(x)} y_i\]</p>

<p>\[\hat{f}(x) = {\rm Ave}(y_i | x_i \in N_k(x))\]</p>

<p><img class="center" src="../../assets/img/knn.png" height=300></p>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>1 nearest neighbor</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/1nn.png" height=400></p>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>KNN for binary problem</h2>
  </hgroup>
  <article data-timings="">
    <p>\[\hat{Pr}(Y=k) = {\rm Ave}(1(y_i == k) | x_i \in N_k(x))\]</p>

<ul>
<li>Advantages

<ul>
<li>Fully non-parametric</li>
<li>Flexible</li>
</ul></li>
<li>Disadvantages

<ul>
<li>Prone to overfitting</li>
<li>Curse of dimensionality</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Logistic regression</h2>
  </hgroup>
  <article data-timings="">
    <p><center> Assume a model for \(Pr(Y=k | X=x)\) </center></p>

<p>\[ \rm{logit}Pr(Y = k |X = x) == x\beta\]</p>

<ul>
<li>Advantages

<ul>
<li>Still doesn&#39;t assume model for \(x\)</li>
<li>Can be more easily explained</li>
</ul></li>
<li>Disadvantages

<ul>
<li>Lose flexibility</li>
<li>Accuracy decreases when linear assumption isn&#39;t true</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Linear discriminant analysis</h2>
  </hgroup>
  <article data-timings="">
    <p><center> Build parametric model for conditional distribution \(P(Y = k | X = x)\) </center></p>

<p>\[Pr(Y = k | X=x) = \frac{f_k(x) \pi_k}{\sum_{\ell = 1}^K f_{\ell}(x) \pi_{\ell}}\]</p>

<ul>
<li>Classify to class with highest estimated probability</li>
<li>Linear discriminant analysis assumes \(f_k(x)\) is multivariate Gaussian with same covariances</li>
<li>Quadratic discrimant analysis assumes \(f_k(x)\) is multivariate Gaussian with different covariances</li>
<li>You can assume more complicated distributions to get more complicated boundaries</li>
<li>Naive Bayes would be assuming independence between features </li>
</ul>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Why &quot;linear&quot; discriminant analysis?</h2>
  </hgroup>
  <article data-timings="">
    <p>\[log \frac{Pr(Y = k | X=x)}{Pr(Y = j | X=x)}\]
\[ = log \frac{f_k(x)}{f_j(x)} + log \frac{\pi_k}{\pi_j}\]
\[ = log \frac{\pi_k}{\pi_j} - \frac{1}{2}(\mu_k + \mu_j)^T \Sigma^{-1}(\mu_k + \mu_j)\]
\[ + x^T \Sigma^{-1} (\mu_k - \mu_j)\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Decision boundaries</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/ldaboundary.png" height=300></p>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Discriminant function</h2>
  </hgroup>
  <article data-timings="">
    <p>\[\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k \Sigma^{-1}\mu_k + log(\mu_k)\]</p>

<ul>
<li>Decide on class based on \(\hat{Y}(x) = argmax_k \delta_k(x)\)</li>
<li>We usually estimate parameters with maximum likelihood</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Logistic regression versus LDA</h2>
  </hgroup>
  <article data-timings="">
    <p>\[log \frac{Pr(Y = k | X=x)}{Pr(Y = j | X=x)}\]
\[ = log \frac{\pi_k}{\pi_j} - \frac{1}{2}(\mu_k + \mu_j)^T \Sigma^{-1}(\mu_k + \mu_j)\]
\[ + x^T \Sigma^{-1} (\mu_k - \mu_j)\]
\[ = \alpha_{k0} + \alpha_k^Tx\]</p>

<p>Similarly from logistic regression</p>

<p>\[log \frac{Pr(Y = k | X=x)}{Pr(Y = j | X=x)} = \beta_{k0} + \beta_k^Tx\]</p>

<p>Both have same form for second term in this equation, but LDA assumes form for first term as well</p>

<p>\[Pr(X,Y=k) = Pr(X)Pr(Y=k|X)\]</p>

<p><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Model based prediction approach</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Assume that prior probability for a cluster is \(\pi_k\)</li>
<li>Assume multivariate density for \(f(x | Y=k)\), usually normal</li>
<li>Build a general model for \(\Sigma_k\) the conditional covariance</li>
<li>Estimate the model with the EM approach</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Choices for covariance</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/mclustcov.png" height=350></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Bayes factors</h2>
  </hgroup>
  <article data-timings="">
    <p>\[B_{12} = \frac{Pr(X|M_1)}{Pr(X|M_2)}\]</p>

<p>where</p>

<p>\[Pr(X | M_k) = \int Pr(X | \theta_k M_k)Pr(\theta_k | M_k)d\theta_k\]</p>

<ul>
<li>Variables are then selected based on which model is &quot;best&quot; by this metric</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>mclust package in R is where this is done</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">library(mclust); data(faithful)
plot(faithful)
</code></pre>

<div class="rimage center"><img src="fig/faithful.png" title="plot of chunk faithful" alt="plot of chunk faithful" class="plot" /></div>

<p><a href="http://www.stat.washington.edu/mclust/">http://www.stat.washington.edu/mclust/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>mclust package in R is where this is done</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">faithfulMclust &lt;- Mclust(faithful)
summary(faithfulMclust)
</code></pre>

<pre><code>----------------------------------------------------
Gaussian finite mixture model fitted by EM algorithm 
----------------------------------------------------

Mclust EEE (elliposidal, equal volume, shape and orientation) model with 3 components:

 log.likelihood   n df   BIC   ICL
          -1126 272 11 -2314 -2361

Clustering table:
  1   2   3 
130  97  45 
</code></pre>

<p><a href="http://www.stat.washington.edu/mclust/">http://www.stat.washington.edu/mclust/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>mclust package in R is where this is done</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">par(mfrow=c(2,2))
plot(faithfulMclust)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-1.png" title="plot of chunk unnamed-chunk-1" alt="plot of chunk unnamed-chunk-1" class="plot" /></div>

<p><a href="http://www.stat.washington.edu/mclust/">http://www.stat.washington.edu/mclust/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Model based clustering advantages and disadvantages</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Advantages

<ul>
<li>Fully parametric so can compute all posterior quantities</li>
<li>Can be a little more interpretable</li>
</ul></li>
<li>Disadvantages

<ul>
<li>Much less flexible</li>
<li>If your assumptions are wrong can be pretty far off</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Bagging proof</h2>
  </hgroup>
  <article data-timings="">
    <p>Let \(\phi(x,D_{train})\) be the predictor based on a training set \(D_{train}\) then we could create a bagged predictor:</p>

<p>\[\phi_A(x,P) = E_{D}[\phi(x,D)]\]</p>

<p>where \(P\) is the probability distribution over \(D\). Then:</p>

<p>\[e = E_D E_{Y,X}(Y-\phi(X,D))^2\]</p>

<p>aggregated error is:</p>

<p>\[e_A = E_{Y,X}(Y-\phi_A(X,D))^2\]</p>

<p>Using the inequality \((EZ)^2 \leq EZ^2\)</p>

<p>\[e = EY^2 - 2EY\phi_A + E_{Y,X}E_D \phi^2(X,D\]
\[ \geq E(Y-\phi_A)^2 = e_A\]</p>

<p><a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Pro tip'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Papers of the day'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='New pro tip'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='New paper of the day'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='An example of least parametric to most'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='KNN neighbors'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='1 nearest neighbor'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='KNN for binary problem'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Logistic regression'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Linear discriminant analysis'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Why &quot;linear&quot; discriminant analysis?'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Decision boundaries'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Discriminant function'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Logistic regression versus LDA'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Model based prediction approach'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Choices for covariance'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Bayes factors'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='mclust package in R is where this is done'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='mclust package in R is where this is done'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='mclust package in R is where this is done'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='Model based clustering advantages and disadvantages'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='Bagging proof'>
         22
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../../librariesNew/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="../../librariesNew/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>