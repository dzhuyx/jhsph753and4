<!DOCTYPE html>
<html>
<head>
  <title>Penalized regression</title>
  <meta charset="utf-8">
  <meta name="description" content="Penalized regression">
  <meta name="author" content="Jeffrey Leek">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="../../librariesNew/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  
  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../librariesNew/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="../../librariesNew/frameworks/io2012/js/slides" 
    src="../../librariesNew/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="../../assets/img/bloomberg_shield.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Penalized regression</h1>
    <h2></h2>
    <p>Jeffrey Leek<br/>Johns Hopkins Bloomberg School of Public Health</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Old pro tip</h2>
  </hgroup>
  <article data-timings="">
    <p>Be judicious in your choice of dependencies (theoretical, software, career). Every dependency you add is one more potential failing point of your (theorem,software,career). That being said, no dependencies is the surest path to disaster. Ramnath whenever possible (build incredible things at high speed by repurposing old tools in the most creative way possible) </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Old paper of the day</h2>
  </hgroup>
  <article data-timings="">
    <p><a href="http://bit.ly/OPlUEM">Automatic decoding of facial movements reveals deceptive pain expressions</a></p>

<p><img class="center" src="../../assets/img/painga.jpg" height="450"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>New pro tip</h2>
  </hgroup>
  <article data-timings="">
    <p>One of the most important components of a research career is building relationships. There are obvious tangible reasons:</p>

<ul>
<li>Grant panels</li>
<li>Tenure review letters</li>
<li>Referees of papers</li>
</ul>

<p>There are also intangible reasons:</p>

<ul>
<li>Learning opportunities</li>
<li>Friendships</li>
</ul>

<p>There is a temptation when &quot;making a name for yourself&quot; to put other people down. In the long run this hurts you way more than you would expect it to. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Blogs of the day</h2>
  </hgroup>
  <article data-timings="">
    <p><a href="http://www.michaeleisen.org/blog/?p=1580">Why I, a founder of PLOS, am forsaking open access</a></p>

<p><a href="http://andrewgelman.com/2014/03/31/cited-statistics-papers-ever/">The most cited statistis papers ever</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Finish testing</li>
<li>Regularization</li>
<li>EM Algorithm</li>
<li>Multiple testing</li>
<li>Empirical Bayes and local false discovery rates</li>
<li>Smoothing 

<ul>
<li>GAMs</li>
<li>Wavelets</li>
<li>Splines</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Agenda (cont.)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Machine learning

<ul>
<li>Random Forests</li>
<li>CART</li>
<li>Model blending</li>
<li>Bagging</li>
<li>Boosting</li>
</ul></li>
<li>Simulation</li>
<li>Causal inference (time permitting)</li>
<li>Time series (time permitting)</li>
<li>Recommender systems (time permitting)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Today: penalized regression</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/lasso.png" height="250"></p>

<p><strong>You should care because</strong></p>

<ul>
<li>Now a standard tool</li>
<li><em>Tons</em> of people work on it</li>
<li>Interesting/clever theory to be done</li>
</ul>

<p><a href="http://statweb.stanford.edu/%7Etibs/lasso/lasso.pdf">http://statweb.stanford.edu/~tibs/lasso/lasso.pdf</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Defining some terms</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p><em>Loss function</em>: \(L \left\{Y,f(X)\right\}\)</p>

<ul>
<li>\(L \left\{Y,f(X)\right\} = \left\{Y-f(X)\right\}^2\)</li>
<li>\(L\left\{Y,f(X)\right\} = |Y-f(X)|\)</li>
<li>\(L\left\{Y,f(X)\right\} = \left\{\begin{array}{cc} 0 & if \; Y=f(X)\\ 1 & else \end{array}\right.\) (binary data)</li>
</ul></li>
<li><p><em>Expected prediction error (EPE)</em>: \(E_X E_{Y|X}\left[\{Y-f(X)\}^2| X\right]\)</p>

<ul>
<li>For squared error loss this is minimized by conditional expectation</li>
<li>\(f(x) = E[Y | X = x]\)</li>
</ul></li>
<li><p><em>Regression function</em>: \(f(x)\)</p></li>
<li><p><em>Training error</em>: \(\frac{1}{N} \sum_{i=1}^N \{y_i - \hat{f}(x_i)\}^2\)</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>An example</h2>
  </hgroup>
  <article data-timings="">
    <p>\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon\]</p>

<p>where \(X_1\) and \(X_2\) are nearly perfectly correlated (co-linear). You can approximate this model by:</p>

<p>\[Y = \beta_0 + (\beta_1 + \beta_2)X_1 + \epsilon\]</p>

<p>The result is:</p>

<ul>
<li>You will get a good estimate of \(Y\)</li>
<li>The estimate (of \(Y\)) will be biased </li>
<li>We may reduce variance in the estimate</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Prostate cancer</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">library(ElemStatLearn); data(prostate)
str(prostate)
</code></pre>

<pre><code>&#39;data.frame&#39;:   97 obs. of  10 variables:
 $ lcavol : num  -0.58 -0.994 -0.511 -1.204 0.751 ...
 $ lweight: num  2.77 3.32 2.69 3.28 3.43 ...
 $ age    : int  50 58 74 58 62 50 64 58 47 63 ...
 $ lbph   : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...
 $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...
 $ lcp    : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...
 $ gleason: int  6 6 7 6 6 6 6 6 6 6 ...
 $ pgg45  : int  0 0 20 0 0 0 0 0 0 0 ...
 $ lpsa   : num  -0.431 -0.163 -0.163 -0.163 0.372 ...
 $ train  : logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Subset selection</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/prostate.png" height="450"></p>

<p><a href="http://www.cbcb.umd.edu/%7Ehcorrada/PracticalML/src/selection.R">Code here</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Most common pattern</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/trainingandtest.png" height="450"></p>

<p><a href="http://www.biostat.jhsph.edu/%7Eririzarr/Teaching/649/">http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Model selection approach: split samples</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>No method better when data/computation time permits it</p></li>
<li><p>Approach</p>

<ol>
<li>Divide data into training/test/validation</li>
<li>Treat validation as test data, train all competing models on the train data and pick the best one on validation. </li>
<li>To appropriately assess performance on new data apply to test set</li>
<li>You may re-split and reperform steps 1-3</li>
</ol></li>
<li><p>Two common problems</p>

<ul>
<li>Limited data</li>
<li>Computational complexity</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Decomposing expected prediction error</h2>
  </hgroup>
  <article data-timings="">
    <p>Assume \(Y_i = f(X_i) + \epsilon_i\)</p>

<p>\(EPE(\lambda) = E\left[\{Y - \hat{f}_{\lambda}(X)\}^2\right]\)</p>

<p>Suppose \(\hat{f}_{\lambda}\) is the estimate from the training data and look at a new data point \(X = x^*\)</p>

<p>\[E\left[\{Y - \hat{f}_{\lambda}(x^*)\}^2\right] = \sigma^2 + \{E[\hat{f}_{\lambda}(x^*)] - f(x^*)\}^2 + var[\hat{f}_\lambda(x_0)]\]</p>

<p><center> = Irreducible error + Bias\(^2\) + Variance </center></p>

<p>\[Err(x_0) = E[(Y - \hat{f}_{\lambda})^2 | X = x_0] = \sigma^2 + \left[f(x_0) - E\hat{f}(x_i)\right]^2 + ||h(x_0)||^2\sigma^2\]</p>

<p>\(h(x_0) = X(X^TX)^{-1}x_0\) is the set of linear weights so \(var[\hat{f}_{\lambda}(x_0)] = ||h(x_0)||^2 \sigma^2\).</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Bias-variance tradeoff</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>This decomposition is known as a bias-variance tradeoff</li>
<li>As the model becomes more complex (more terms) you reduce bias by getting local structure/curvature</li>
<li>But coefficient estimates suffer from high variance for more terms/colinear terms</li>
<li>So introducing a little bias in our estimate for \(\beta\) might lead to substantial decrease in MSE</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Hard thresholding</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Model \(Y = f(X) + \epsilon\)</p></li>
<li><p>Set \(\hat{f}_{\lambda}(x) = x'\beta\)</p></li>
<li><p>Constrain only \(\lambda\) coefficients to be nonzero. </p></li>
<li><p>Selection problem is after chosing \(\lambda\) figure out which \(p - \lambda\) coefficients to make nonzero</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Shrinkage and penalties</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The classic shrinkage result is the James-Stein estimator. Suppose \(Y_i \sim N(\theta, \sigma^2 I)\)
where we would like to estimate \(\hat{\theta}\) on the basis of the \(Y\) variables and \(\sigma^2\) is known. </p></li>
<li><p>Then we know what estimator we would usually choose \(\hat{\theta} = \bar{Y}\). </p></li>
<li><p>Stein showed that if the goal is to minimize the mean squared error \(\e[(\theta - \hat{\theta})^2]\) and we have \(n \geq 3\) observations, we can always do better than the mean with estimators like this:
\[\hat{\theta}_{JS} = \left(1-\frac{(m-2)\sigma^2}{||\bar{Y}||^2}\right)\bar{Y}\] </p></li>
<li><p>Even more surprising, if we consider any direction \(\nu\) 
\[\hat{\theta}_{JS}=\left(1-\frac{(m-2)\sigma^2}{||\bar{Y} - \nu||^2}\right)(\bar{Y} - \nu) + \nu\]
also has lower MSE than the mean!!</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Shrinkage and penalties</h2>
  </hgroup>
  <article data-timings="">
    <p>Shrinkage can be thought of as &quot;constrained&quot; minimization.\vsp
Minimize:</p>

<p>\(\sum_{i=1}^n (Y_i - \mu)^2\) subject to \(\mu^2 \leq c\)</p>

<p>Differentiating: </p>

<p>\[-2 \sum_{i=1}^n (Y_i - \hat{\mu}_c) + 2\lambda_c \hat{\mu}_c = 0\]</p>

<p>So </p>

<p>\(\hat{\mu}_c = \frac{\sum_{i=1}^nY_i}{n + \lambda_c} = K_c\bar{Y}\) where \(K_c < 1\)</p>

<p>The precise form of \(\lambda_c\) is unimportant: as \(c \rightarrow 0\), \(\hat{\mu} \rightarrow \bar{Y}\), as \(c \rightarrow \infty\), \(\hat{\mu}_c \rightarrow 0\). </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Shrinkage and penalties</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Giving up bias to reduce variance</li>
<li>Not all biased models are better - we need to find ``good&#39;&#39; biased models</li>
<li>Generalized one-sample problem: penalize large values of \(\beta\) this should lead to ``multivariate&#39;&#39; shrinkage</li>
<li>Heuristically, many \(\beta > 0\) is interpreted as complex model.</li>
<li>For good penalties there is often an equivalent Bayesian interpretation</li>
<li>If the truth is really complex, this may not work! (But it will then be hard to build a good model anyway...)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Another issue for high-dimensional data</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">small = prostate[1:5,]
lm(lpsa ~ .,data =small)
</code></pre>

<pre><code>
Call:
lm(formula = lpsa ~ ., data = small)

Coefficients:
(Intercept)       lcavol      lweight          age         lbph          svi          lcp  
     9.6061       0.1390      -0.7914       0.0952           NA           NA           NA  
    gleason        pgg45    trainTRUE  
    -2.0871           NA           NA  
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>Regularization for regression</h2>
  </hgroup>
  <article data-timings="">
    <p>If the \(\beta_j\)&#39;s are unconstrained:</p>

<ul>
<li>They can explode</li>
<li>And hence are susceptible to very high variance</li>
</ul>

<p>To control variance, we might regularize/shrink the coefficients. </p>

<p>\[ PRSS(\beta) = \sum_{j=1}^n (Y_j - \sum_{i=1}^m \beta_{1i} X_{ij})^2 + P(\lambda; \beta)\]</p>

<p>where \(PRSS\) is a penalized form of the sum of squares. Things that are commonly looked for</p>

<ul>
<li>Penalty reduces complexity</li>
<li>Penalty reduces variance</li>
<li>Penalty respects structure of the problem</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>Ridge regression</h2>
  </hgroup>
  <article data-timings="">
    <p>Solve:</p>

<p>\[ \sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p \beta_j^2\]</p>

<p>equivalent to solving</p>

<p>\(\sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2\) subject to \(\sum_{j=1}^p \beta_j^2 \leq s\) where \(s\) is inversely proportional to \(\lambda\) </p>

<p>Solution is:</p>

<p>\[\hat{\beta}^{ridge} = (X^TX + \lambda I)^{-1} X^T y\]</p>

<p>Inclusion of \(\lambda\) makes the problem non-singular even if \(X^TX\) is not invertible. (this was the original motivation for ridge regression - see Hoerl and Kennard 1970)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h2>Ridge coefficient paths</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/ridgepath.png" height="450"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Tuning parameter \(\lambda\)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>\(\lambda\) controls the size of the coefficients</li>
<li>\(\lambda\) controls the amount of {\bf regularization}</li>
<li>As \(\lambda \rightarrow 0\) we obtain the least square solution</li>
<li>As \(\lambda \rightarrow \infty\) we have \(\hat{\beta}_{\lambda=\infty}^{ridge} = 0\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>SVD, PCA and ridge regression</h2>
  </hgroup>
  <article data-timings="">
    <p>\[X = UDV^T\]</p>

<p>\[\hat{y} = X\hat{\beta} X(X^TX)^{-1}X^Ty = UU^Ty\]</p>

<p>\[ X\hat{\beta}^{ridge} = X(X^TX + \lambda I)^{-1} X y\]</p>

<p>\[ = UD(D^2 + \lambda I)^{-1} DU' y\]</p>

<p>\[ = \sum_{j=1}^p u_j \frac{d_j}{d_j + \lambda} u_j^Ty\]</p>

<p>Notice that \(\lambda > 0, \frac{d_j}{d_j  + \lambda} \leq 1\). </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Lasso</h2>
  </hgroup>
  <article data-timings="">
    <p>\(\sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2\) subject to \(\sum_{j=1}^p |\beta_j| \leq s\) </p>

<p>also has a lagrangian form </p>

<p>\[ \sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p |\beta_j|\]</p>

<p>For orthonormal design matrices (not the norm!) this has a closed form solution</p>

<p>\[\hat{\beta}_j = sign(\hat{\beta}_j^0)(|\hat{\beta}_j^0 - \gamma)^{+}\]</p>

<p>but not in general. </p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Computing lasso solutions -least angle regression</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/lars.png" height="450"></p>

<p><a href="http://www.stanford.edu/%7Ehastie/Papers/LARS/LeastAngle_2002.pdf">http://www.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf</a></p>

<p><a href="http://cran.r-project.org/web/packages/lars/index.html">lars package</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h2>Why does the lasso produce zeros?</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/hanlasso.png" height="450"></p>

<p><a href="http://www.princeton.edu/%7Ehanliu/">Image via Han Liu</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h2>Forward stagewise</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Start with residual \(R = Y\) and \(\beta_j = 0 \forall j\)</li>
<li>Find the predictor \(X_j\) most correlated with \(R\)</li>
<li>Update \(\beta_j = \beta_j + \delta\) where \(\delta_j = e \times sign(X^T_j R)\)</li>
<li>Set \(R \leftarrow R - \delta_j X_j\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h2>Paths of lasso/lars/stagewise in R</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">library(lars); data(diabetes); library(MASS); dim(lars1$x)
length(diabetes$y)
mod1 = lars(diabetes$x,diabetes$y,&quot;lasso&quot;)
mod2 = lars(diabetes$x,diabetes$y,&quot;forward.stagewise&quot;)
mod3 = lm.ridge(diabetes$y ~ diabetes$x,lambda=seq(0,2e4,by=20))
par(mfrow=c(1,3))
matplot(cbind(apply(abs(mod1$beta),1,sum)),mod1$beta,col=&quot;blue&quot;,type=&quot;l&quot;,
        lwd=3,xlab=&quot;Sum of absolute betas&quot;, ylab=&quot;beta hat&quot;)
matplot(cbind(apply(abs(mod2$beta),1,sum)),mod2$beta,col=&quot;red&quot;,type=&quot;l&quot;,
        lwd=3,xlab=&quot;Sum of absolute betas&quot;, ylab=&quot;beta hat&quot;)
matplot(cbind(apply(abs(mod3$coef),2,sum)),t(mod3$coef),type=&quot;l&quot;,col=&quot;purple&quot;,
        lwd=3,xlab=&quot;Sum of absolute betas&quot;, ylab=&quot;beta hat&quot;)

</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h2>Coefficient path comparison</h2>
  </hgroup>
  <article data-timings="">
    <p><img class="center" src="../../assets/img/pathcomp.png" height="450"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h2>I prefer simpler in many cases</h2>
  </hgroup>
  <article data-timings="">
    <p><center> But doesn&#39;t work well with many correlated and good predictors! </center></p>

<p><img class="center" src="../../assets/img/leekasso.png" height="400"></p>

<p><a href="http://simplystatistics.org/2014/01/04/repost-prediction-the-lasso-vs-just-using-the-top-10-predictors/">http://simplystatistics.org/2014/01/04/repost-prediction-the-lasso-vs-just-using-the-top-10-predictors/</a>
<a href="http://simplystatistics.org/2014/03/12/oh-no-the-leekasso/">http://simplystatistics.org/2014/03/12/oh-no-the-leekasso/</a></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Lecture slides credits</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><a href="http://www.cbcb.umd.edu/%7Ehcorrada/PracticalML/">Hector Corrada Bravo&#39;s Practical Machine Learning lecture notes</a></li>
<li><a href="http://www.cbcb.umd.edu/%7Ehcorrada/AMSC689.html#readings">Hector&#39;s penalized regression reading list</a></li>
<li><a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">Elements of Statistical Learning</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Old pro tip'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Old paper of the day'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='New pro tip'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Blogs of the day'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Agenda'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Agenda (cont.)'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Today: penalized regression'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Defining some terms'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='An example'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Prostate cancer'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Subset selection'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Most common pattern'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Model selection approach: split samples'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Decomposing expected prediction error'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Bias-variance tradeoff'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Hard thresholding'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Shrinkage and penalties'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='Shrinkage and penalties'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='Shrinkage and penalties'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='Another issue for high-dimensional data'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='Regularization for regression'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='Ridge regression'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='Ridge coefficient paths'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='Tuning parameter \(\lambda\)'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='SVD, PCA and ridge regression'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='Lasso'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='Computing lasso solutions -least angle regression'>
         27
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=28 title='Why does the lasso produce zeros?'>
         28
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=29 title='Forward stagewise'>
         29
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=30 title='Paths of lasso/lars/stagewise in R'>
         30
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=31 title='Coefficient path comparison'>
         31
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=32 title='I prefer simpler in many cases'>
         32
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=33 title='Lecture slides credits'>
         33
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../../librariesNew/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="../../librariesNew/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>